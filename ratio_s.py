import streamlit as st
import cv2
import mediapipe as mp
import time
import numpy as np
import av
import queue
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
# â­ ì†Œë¦¬ ì¬ìƒì„ ìœ„í•´ components ëª¨ë“ˆì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from streamlit import components

# ---------------- Mediapipe ì´ˆê¸°í™” ----------------
mp_face = mp.solutions.face_detection
mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

FACE_DETECTOR = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.4)
HAND_DETECTOR = mp_hands.Hands(max_num_hands=2, min_detection_confidence=0.5)

# ---------------- ì›¹ ì•± ì „ì—­ ë³€ìˆ˜ ì„¤ì • ----------------
TARGET_A_MIN, TARGET_A_MAX = 43, 47
TARGET_B_MIN, TARGET_B_MAX = 12, 15
COUNTDOWN_TIME = 3.0  # ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œê°„ ì„¤ì •

# STUN ì„œë²„ (ê²€ì€ í™”ë©´ ë°©ì§€)
RTC_CONFIGURATION = RTCConfiguration(
    {"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}
)


# ---------------- Victory ì œìŠ¤ì²˜ íŒë‹¨ ----------------
def is_victory(lms, w, h):
    """ê²€ì§€+ì¤‘ì§€ í´ì§, ì•½ì§€+ìƒˆë¼ ì ‘í˜ì´ë©´ V ì‚¬ì¸ True"""
    try:
        # ì†ê°€ë½ ë(tip)ì´ ë§ˆë””(knuckle)ë³´ë‹¤ ìœ„(yì¢Œí‘œ ì‘ìŒ)ì— ìˆìœ¼ë©´ í´ì§
        return (lms.landmark[8].y < lms.landmark[5].y and
                lms.landmark[12].y < lms.landmark[9].y and
                lms.landmark[16].y > lms.landmark[13].y and
                lms.landmark[20].y > lms.landmark[17].y)
    except:
        return False


# ---------------- ë¹„ìœ¨ ê³„ì‚° í•¨ìˆ˜ ----------------
def get_face_distances(detection, img_h):
    keypoints = detection.location_data.relative_keypoints
    bbox_h = detection.location_data.relative_bounding_box.height

    if bbox_h == 0:
        return {'eye_mouth_ratio': 0.0, 'nose_mouth_ratio': 0.0}

    y_eye_r = keypoints[1].y
    y_eye_l = keypoints[0].y
    y_eye_center = (y_eye_r + y_eye_l) / 2
    y_mouth = keypoints[3].y
    y_nose = keypoints[2].y

    distance_eye_mouth_norm = abs(y_mouth - y_eye_center)
    eye_mouth_ratio = distance_eye_mouth_norm / bbox_h

    distance_nose_mouth_norm = abs(y_mouth - y_nose)
    nose_mouth_ratio = distance_nose_mouth_norm / bbox_h

    return {
        'eye_mouth_ratio': eye_mouth_ratio,
        'nose_mouth_ratio': nose_mouth_ratio
    }


# ---------------- ê²Œì´ì§€ ê·¸ë¦¬ê¸° í•¨ìˆ˜ ----------------
def draw_gauge(img, ratio_percent, x_offset, target_min, target_max, label):
    """í™”ë©´ ì™¼ìª½ì— ìˆ˜ì§ ê²Œì´ì§€ë¥¼ ê·¸ë¦½ë‹ˆë‹¤."""
    gauge_x, gauge_y = 50 + x_offset, 80
    gauge_w, gauge_h = 20, 200

    ratio_percent_clamped = max(0, min(100, ratio_percent))

    is_target = target_min <= ratio_percent_clamped <= target_max

    target_color = (0, 255, 0)
    base_color = (255, 255, 255)
    fill_color = target_color if is_target else (0, 0, 255)

    cv2.rectangle(img, (gauge_x, gauge_y), (gauge_x + gauge_w, gauge_y + gauge_h), base_color, 2)

    fill_height = int(gauge_h * (ratio_percent_clamped / 100))
    fill_y_start = gauge_y + gauge_h - fill_height
    cv2.rectangle(img, (gauge_x, fill_y_start), (gauge_x + gauge_w, gauge_y + gauge_h), fill_color, cv2.FILLED)

    y_max = gauge_y + gauge_h - int(gauge_h * (target_min / 100))
    y_min = gauge_y + gauge_h - int(gauge_h * (target_max / 100))

    cv2.rectangle(img, (gauge_x, y_min), (gauge_x + gauge_w, y_max), (0, 255, 255), 1)

    cv2.putText(img, label, (gauge_x - 10, gauge_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, base_color, 1)
    cv2.putText(img, f"{ratio_percent_clamped}%", (gauge_x - 10, gauge_y + gauge_h + 30),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, base_color, 2)

    return is_target, ratio_percent_clamped


# ---------------- VideoProcessor í´ë˜ìŠ¤ (í•µì‹¬) ----------------
class VideoProcessor(VideoTransformerBase):

    def __init__(self):
        self.result_queue = queue.Queue()
        # â­ ì…”í„° ì†Œë¦¬ ì „ìš© í ì¶”ê°€
        self.shutter_queue = queue.Queue()

        self.captured = False
        self.last_capture_time = 0
        self.countdown_active = False
        self.countdown_start_time = 0
        self.face_detector = FACE_DETECTOR
        self.hand_detector = HAND_DETECTOR

    def recv(self, frame: av.VideoFrame) -> av.VideoFrame:
        img = frame.to_ndarray(format="bgr24")
        img_h, img_w, _ = img.shape
        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        if self.captured:
            cv2.putText(img, "CAPTURED! (Hold)", (img_w // 2 - 150, img_h // 2 + 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 3)
            return av.VideoFrame.from_ndarray(img, format="bgr24")

        # ---------------- 1. ì–¼êµ´/ì† ì¸ì‹ ë° ë¹„ìœ¨ í™•ì¸ ----------------
        face_res = self.face_detector.process(rgb)
        face_detected = False
        ratio_ok_A, ratio_ok_B = False, False
        ratio_A_percent, ratio_B_percent = 0, 0
        victory_detected = False

        img_out = img.copy()

        if face_res.detections:
            face_detected = True
            d = face_res.detections[0]
            current_ratios = get_face_distances(d, img_h)
            ratio_A_percent = int(current_ratios['eye_mouth_ratio'] * 100)
            ratio_B_percent = int(current_ratios['nose_mouth_ratio'] * 100)
            ratio_ok_A = TARGET_A_MIN <= ratio_A_percent <= TARGET_A_MAX
            ratio_ok_B = TARGET_B_MIN <= ratio_B_percent <= TARGET_B_MAX
            mp_draw.draw_detection(img_out, d)

        hand_res = self.hand_detector.process(rgb)
        if hand_res.multi_hand_landmarks:
            for handLms in hand_res.multi_hand_landmarks:
                if is_victory(handLms, img_w, img_h):
                    victory_detected = True
                mp_draw.draw_landmarks(img_out, handLms, mp_hands.HAND_CONNECTIONS)

        # ---------------- 2. ê²Œì´ì§€ í‘œì‹œ ----------------
        draw_gauge(img_out, ratio_A_percent, 0, TARGET_A_MIN, TARGET_A_MAX, "E-M Ratio")
        draw_gauge(img_out, ratio_B_percent, 70, TARGET_B_MIN, TARGET_B_MAX, "N-M Ratio")

        total_ratio_ok = ratio_ok_A and ratio_ok_B
        all_conditions_met = face_detected and victory_detected and total_ratio_ok

        # ---------------- 3. ì¹´ìš´íŠ¸ë‹¤ìš´ ë° ìº¡ì²˜ ë¡œì§ ----------------
        if all_conditions_met:
            if not self.countdown_active:
                self.countdown_active = True
                self.countdown_start_time = time.time()
                st.session_state.capture_message = f"ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œì‘! {COUNTDOWN_TIME}ì´ˆ ìœ ì§€í•˜ì„¸ìš”."

            elapsed = time.time() - self.countdown_start_time
            countdown_value = COUNTDOWN_TIME - elapsed

            if countdown_value <= 0:
                self.countdown_active = False
                self.captured = True
                self.last_capture_time = time.time()

                self.result_queue.put(rgb)

                # â­ ìº¡ì²˜ ì„±ê³µ ì‹œ ì…”í„° ì†Œë¦¬ ì‹ í˜¸ ì „ì†¡
                try:
                    self.shutter_queue.put(True, block=False)
                except queue.Full:
                    pass

                print("!!! ìº¡ì²˜ ì„±ê³µ! íì— ì´ë¯¸ì§€ ì „ì†¡ë¨ !!!")

        else:
            if self.countdown_active:
                self.countdown_active = False
                st.session_state.capture_message = "â³ ì¡°ê±´ ë¯¸ë‹¬ë¡œ ì¹´ìš´íŠ¸ë‹¤ìš´ ì¤‘ë‹¨."

            if not self.captured and not self.countdown_active:
                st.session_state.capture_message = "ì¡°ê±´ì„ ì¶©ì¡±ì‹œì¼œì£¼ì„¸ìš”."

        # ---------------- 4. í™”ë©´ í‘œì‹œ ê°±ì‹  ----------------
        if self.countdown_active:
            countdown_display = max(1, int(COUNTDOWN_TIME - (time.time() - self.countdown_start_time)) + 1)
            cv2.putText(img_out, f"Capturing in: {countdown_display}", (img_w // 2 - 150, img_h // 2),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 255), 4)

        status_text = (
            f"Face: {face_detected} | V: {victory_detected} | "
            f"Ratio A: {ratio_ok_A} | Ratio B: {ratio_ok_B}"
        )
        cv2.putText(img_out, status_text,
                    (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)

        return av.VideoFrame.from_ndarray(img_out, format="bgr24")


# ---------------- Streamlit ë©”ì¸ í•¨ìˆ˜ ----------------
def main():
    st.set_page_config(page_title="ë¹„ìœ¨ & V ì‚¬ì¸ ê²€ì¶œê¸°", layout="wide")

    st.title("ğŸ“¸ ë¹„ìœ¨ ìµœì í™” V-ì‚¬ì¸ ìë™ ìº¡ì²˜ ì›¹ ì•±")
    st.markdown("""
    ëª¨ë“  ì¡°ê±´ì´ ì¶©ì¡±ë˜ë©´ **3ì´ˆ ì¹´ìš´íŠ¸ë‹¤ìš´** í›„ ìë™ìœ¼ë¡œ ìº¡ì²˜ë˜ë©°, ì…”í„° ì†Œë¦¬ê°€ ë‚©ë‹ˆë‹¤.
    ---
    """)

    # â­ MIME íƒ€ì…ê³¼ static ê²½ë¡œë¥¼ ëª…ì‹œí•œ ìµœì¢… HTML ì½”ë“œ
    SHUTTER_HTML = """
    <audio id="shutter_sound" preload="auto">
      <source src="static/shutter.wav" type="audio/wav">
    </audio>
    <script>
    var audio = document.getElementById('shutter_sound');
    audio.currentTime = 0;

    // Promiseë¥¼ ì‚¬ìš©í•˜ì—¬ ì¬ìƒì„ ì‹œë„í•˜ê³  ì‹¤íŒ¨ ì‹œ ì½˜ì†”ì— ì˜¤ë¥˜ë¥¼ ëª…í™•íˆ ì¶œë ¥
    var playPromise = audio.play();

    if (playPromise !== undefined) {
      playPromise.then(function() {
        // ì¬ìƒ ì„±ê³µ
      }).catch(function(error) {
        // ì¬ìƒ ì‹¤íŒ¨ (ìë™ ì¬ìƒ ì°¨ë‹¨, ê¶Œí•œ ë¬¸ì œ ë“±)
        console.error("Audio Playback Failed (Source Error/Policy):", error);
      });
    }
    </script>
    """

    # Session State ì´ˆê¸°í™”
    if 'capture_ready' not in st.session_state:
        st.session_state.capture_ready = False
        st.session_state.captured_image_rgb = None
        st.session_state.capture_message = "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”."

    current_message = st.session_state.get('capture_message', "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”.")

    col1, col2 = st.columns([2, 1])

    # ---------------- I. ì›¹ìº  ìŠ¤íŠ¸ë¦¼ (col1) ----------------
    with col1:
        st.subheader("ì‹¤ì‹œê°„ ì›¹ìº  ìŠ¤íŠ¸ë¦¼ (ë¹„ì „ ì²˜ë¦¬)")

        webrtc_ctx = webrtc_streamer(
            key="media-pipe-detector",
            video_processor_factory=VideoProcessor,
            rtc_configuration=RTC_CONFIGURATION,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

        if webrtc_ctx.state.playing:
            if "ì´¬ì˜ ì„±ê³µ" in current_message or st.session_state.get('capture_ready'):
                st.success(f"í˜„ì¬ ìƒíƒœ: **{current_message}**")
            elif "ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œì‘" in current_message:
                st.warning(f"í˜„ì¬ ìƒíƒœ: **{current_message}**")
            else:
                st.info(f"í˜„ì¬ ìƒíƒœ: **{current_message}**")
        else:
            st.warning("ì¹´ë©”ë¼ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")

    # â­ II. ìº¡ì²˜ ë° ì…”í„° ì†Œë¦¬ ê°ì§€ ë£¨í”„ (ë©”ì¸ ìŠ¤ë ˆë“œ)
    if webrtc_ctx.state.playing and webrtc_ctx.video_processor:

        processor = webrtc_ctx.video_processor

        while True:
            try:
                # í íƒ€ì„ì•„ì›ƒì„ 0.1ì´ˆë¡œ ì„¤ì •
                result_img_rgb = processor.result_queue.get(timeout=0.1)
            except queue.Empty:
                result_img_rgb = None
            except Exception as e:
                # print(f"í ê°ì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break

            # â­ ì…”í„° ì†Œë¦¬ ì‹ í˜¸ í™•ì¸ ë° HTML ì‚½ì…
            try:
                if processor.shutter_queue.get(timeout=0.1):  # í íƒ€ì„ì•„ì›ƒì„ 0.1ì´ˆë¡œ ì„¤ì •
                    # ì‹ í˜¸ê°€ ì˜¤ë©´ HTML ì»´í¬ë„ŒíŠ¸ë¥¼ ì‚½ì…í•˜ì—¬ ì†Œë¦¬ ì¬ìƒ
                    components.v1.html(SHUTTER_HTML, height=0)
            except queue.Empty:
                pass
            except Exception:
                pass

            if result_img_rgb is not None:
                st.session_state.captured_image_rgb = result_img_rgb
                st.session_state.capture_ready = True
                st.session_state.capture_message = "âœ… ì´¬ì˜ ì„±ê³µ! ì•„ë˜ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”."

                processor.captured = False

                st.rerun()
                break

            time.sleep(0.01)

    # ---------------- III. ê²°ê³¼ í‘œì‹œ (col2) ----------------
    with col2:
        st.subheader("âœ… ìº¡ì²˜ ì¡°ê±´ ë° ê²°ê³¼")
        st.markdown(
            f"""
            **âœ… ìº¡ì²˜ ì¡°ê±´ (ëª¨ë‘ ì¶©ì¡±í•´ì•¼ í•¨):**
            * **ì–¼êµ´ ê°ì§€** (Face Detected)
            * **V-ì‚¬ì¸ ê°ì§€** (Victory Gesture)
            * **ëˆˆ-ì… ë¹„ìœ¨ (A):** ${TARGET_A_MIN}\% \sim {TARGET_A_MAX}\%$
            * **ì½”-ì… ë¹„ìœ¨ (B):** ${TARGET_B_MIN}\% \sim {TARGET_B_MAX}\%$
            """
        )
        st.markdown("---")

        if st.session_state.get('capture_ready') and st.session_state.get('captured_image_rgb') is not None:
            st.success("ğŸ‰ **ìº¡ì²˜ ì™„ë£Œ!**")

            captured_img = st.session_state.captured_image_rgb

            st.image(captured_img, caption="ìµœê·¼ ìº¡ì²˜ ì´ë¯¸ì§€", use_container_width=True)

            img_bgr = cv2.cvtColor(captured_img, cv2.COLOR_RGB2BGR)
            ret, buffer = cv2.imencode(".png", img_bgr)

            if ret:
                st.download_button(
                    label="ğŸ–¼ï¸ ìº¡ì²˜ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ",
                    data=buffer.tobytes(),
                    file_name=f"capture_optimal_{int(time.time())}.png",
                    mime="image/png"
                )

            if st.button("ğŸ”„ ë‹¤ìŒ ìº¡ì²˜ ì¤€ë¹„"):
                st.session_state.capture_ready = False
                st.session_state.captured_image_rgb = None
                st.session_state.capture_message = "ì¹´ë©”ë¼ë¥¼ ì¼œê³  ìì„¸ë¥¼ ì¡ì•„ì£¼ì„¸ìš”."
                st.rerun()

        else:
            st.warning("ì•„ì§ ìº¡ì²˜ëœ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì¡°ê±´ì„ ì¶©ì¡±ì‹œì¼œë³´ì„¸ìš”!")


if __name__ == "__main__":
    main()